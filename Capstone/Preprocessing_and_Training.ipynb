{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "167b2fdd",
   "metadata": {},
   "source": [
    "The Effects of the Covid-19 Pandemic on Various Life Factors in Students\n",
    "\n",
    "So far, in the Data Wrangling phase, we have been able to determine that a significant need exists for interventions in schools to help students to move past the negative affects of the Covid-19 pandemic. Particularly, relationships and school stress were factors that were siginificantly affected. In the next phase of our study, we will aim to develop a machine learning model to predict the effectiveness of some interventions that have already been used to help adolescents post-pandemic. The data cleaning process addressed missing values and categorical variables. Feature engineering included data type checks and creation of a preprocessor for numerical scaling. We explored various classification models, including decision trees, random forests, gradient boosting, and logistic regression. Hyperparameter tuning significantly improved model performance, highlighting the importance of optimizing model parameters. Cross-validation ensured the model's generalizability beyond the training data.Along the way, we did encounter an imbalance in classifications, but we worked to address the issue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af01cd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "\n",
    "# Disable all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# reset warnings to default behavior\n",
    "warnings.resetwarnings()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e59d21b",
   "metadata": {},
   "source": [
    "First, let's load the original dataset, which demonstrated a need for interventions within the schools to help students post-pandemic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba368d08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Country</th>\n",
       "      <th>State</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Before-Environment</th>\n",
       "      <th>Before-ClassworkStress</th>\n",
       "      <th>Before-HomeworkStress</th>\n",
       "      <th>Before-HomeworkHours</th>\n",
       "      <th>Now-Environment</th>\n",
       "      <th>Now-ClassworkStress</th>\n",
       "      <th>Now-HomeworkStress</th>\n",
       "      <th>Now-HomeworkHours</th>\n",
       "      <th>FamilyRelationships</th>\n",
       "      <th>FriendRelationships</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SchoolCollegeTraining</td>\n",
       "      <td>US</td>\n",
       "      <td>TX</td>\n",
       "      <td>14</td>\n",
       "      <td>Male</td>\n",
       "      <td>Physical</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Virtual</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SchoolCollegeTraining</td>\n",
       "      <td>US</td>\n",
       "      <td>MD</td>\n",
       "      <td>13</td>\n",
       "      <td>Male</td>\n",
       "      <td>Physical</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Virtual</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Homeschool</td>\n",
       "      <td>US</td>\n",
       "      <td>TX</td>\n",
       "      <td>16</td>\n",
       "      <td>Female</td>\n",
       "      <td>Virtual</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Virtual</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SchoolCollegeTraining</td>\n",
       "      <td>US</td>\n",
       "      <td>GA</td>\n",
       "      <td>17</td>\n",
       "      <td>Male</td>\n",
       "      <td>Physical</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Physical</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SchoolCollegeTraining</td>\n",
       "      <td>GB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14</td>\n",
       "      <td>Male</td>\n",
       "      <td>Physical</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Physical</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Category Country State  Age  Gender Before-Environment  \\\n",
       "0  SchoolCollegeTraining      US    TX   14    Male           Physical   \n",
       "1  SchoolCollegeTraining      US    MD   13    Male           Physical   \n",
       "2             Homeschool      US    TX   16  Female            Virtual   \n",
       "3  SchoolCollegeTraining      US    GA   17    Male           Physical   \n",
       "4  SchoolCollegeTraining      GB   NaN   14    Male           Physical   \n",
       "\n",
       "   Before-ClassworkStress  Before-HomeworkStress  Before-HomeworkHours  \\\n",
       "0                       1                      3                   2.0   \n",
       "1                       5                      4                   2.0   \n",
       "2                       1                      3                  10.0   \n",
       "3                       4                      4                   6.0   \n",
       "4                       3                      4                   4.0   \n",
       "\n",
       "  Now-Environment  Now-ClassworkStress  Now-HomeworkStress  Now-HomeworkHours  \\\n",
       "0         Virtual                    3                   5                4.5   \n",
       "1         Virtual                    3                   5                2.5   \n",
       "2         Virtual                    3                   5               15.0   \n",
       "3        Physical                    5                   1                6.0   \n",
       "4        Physical                    5                   5                6.0   \n",
       "\n",
       "   FamilyRelationships  FriendRelationships  \n",
       "0                    2                   -1  \n",
       "1                    1                   -2  \n",
       "2                    1                   -1  \n",
       "3                    0                   -2  \n",
       "4                    0                    1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the data\n",
    "# Get the path to the \"documents\" folder\n",
    "documents_folder = os.path.expanduser('~/Documents')\n",
    "\n",
    "# Specify the file name\n",
    "file_name = 'Covid_Responses.csv'\n",
    "\n",
    "# Construct the full file path\n",
    "file_path = os.path.join(documents_folder, file_name)\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Confirm the dataset loaded properly\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397fa7a5",
   "metadata": {},
   "source": [
    "Now, let's load, examine, and clean (if needed) a dataset in which various interventions were implemented among adolescents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1410921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 235 entries, 0 to 234\n",
      "Data columns (total 17 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   Author     20 non-null     object \n",
      " 1   Group      40 non-null     object \n",
      " 2   T0         0 non-null      float64\n",
      " 3   Scale      177 non-null    object \n",
      " 4   Measure    177 non-null    object \n",
      " 5   SD         177 non-null    object \n",
      " 6   N          31 non-null     float64\n",
      " 7   T1         38 non-null     object \n",
      " 8   Scale.1    235 non-null    object \n",
      " 9   Measure.1  235 non-null    object \n",
      " 10  SD.1       235 non-null    object \n",
      " 11  N.1        40 non-null     float64\n",
      " 12  T2         5 non-null      object \n",
      " 13  Scale.2    20 non-null     object \n",
      " 14  Measure.2  20 non-null     float64\n",
      " 15  SD.2       20 non-null     float64\n",
      " 16  N.2        5 non-null      float64\n",
      "dtypes: float64(6), object(11)\n",
      "memory usage: 31.3+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Author</th>\n",
       "      <th>Group</th>\n",
       "      <th>T0</th>\n",
       "      <th>Scale</th>\n",
       "      <th>Measure</th>\n",
       "      <th>SD</th>\n",
       "      <th>N</th>\n",
       "      <th>T1</th>\n",
       "      <th>Scale.1</th>\n",
       "      <th>Measure.1</th>\n",
       "      <th>SD.1</th>\n",
       "      <th>N.1</th>\n",
       "      <th>T2</th>\n",
       "      <th>Scale.2</th>\n",
       "      <th>Measure.2</th>\n",
       "      <th>SD.2</th>\n",
       "      <th>N.2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cataldi</td>\n",
       "      <td>Control-Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BMI</td>\n",
       "      <td>22.48</td>\n",
       "      <td>2.2</td>\n",
       "      <td>15.0</td>\n",
       "      <td>Immediately</td>\n",
       "      <td>BMI</td>\n",
       "      <td>22.45</td>\n",
       "      <td>2.12</td>\n",
       "      <td>15.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Waist circumference (cm)</td>\n",
       "      <td>74.87</td>\n",
       "      <td>7.59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Waist circumference (cm)</td>\n",
       "      <td>74.9</td>\n",
       "      <td>7.53</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Squat test (rep)</td>\n",
       "      <td>28.89</td>\n",
       "      <td>2.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Squat test (rep)</td>\n",
       "      <td>29.4</td>\n",
       "      <td>2.56</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Push-up test (rep)</td>\n",
       "      <td>9.13</td>\n",
       "      <td>4.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Push-up test (rep)</td>\n",
       "      <td>9.53</td>\n",
       "      <td>4.34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lunge test (rep)</td>\n",
       "      <td>31.13</td>\n",
       "      <td>5.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lunge test (rep)</td>\n",
       "      <td>31.4</td>\n",
       "      <td>6.07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Author      Group  T0                     Scale Measure    SD     N  \\\n",
       "0  Cataldi  Control-Y NaN                      BMI    22.48   2.2  15.0   \n",
       "1      NaN        NaN NaN  Waist circumference (cm)   74.87  7.59   NaN   \n",
       "2      NaN        NaN NaN          Squat test (rep)   28.89   2.4   NaN   \n",
       "3      NaN        NaN NaN        Push-up test (rep)    9.13   4.2   NaN   \n",
       "4      NaN        NaN NaN          Lunge test (rep)   31.13   5.4   NaN   \n",
       "\n",
       "            T1                   Scale.1 Measure.1  SD.1   N.1   T2 Scale.2  \\\n",
       "0  Immediately                      BMI      22.45  2.12  15.0  NaN     NaN   \n",
       "1          NaN  Waist circumference (cm)      74.9  7.53   NaN  NaN     NaN   \n",
       "2          NaN          Squat test (rep)      29.4  2.56   NaN  NaN     NaN   \n",
       "3          NaN        Push-up test (rep)      9.53  4.34   NaN  NaN     NaN   \n",
       "4          NaN          Lunge test (rep)      31.4  6.07   NaN  NaN     NaN   \n",
       "\n",
       "   Measure.2  SD.2  N.2  \n",
       "0        NaN   NaN  NaN  \n",
       "1        NaN   NaN  NaN  \n",
       "2        NaN   NaN  NaN  \n",
       "3        NaN   NaN  NaN  \n",
       "4        NaN   NaN  NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the data\n",
    "# Get the path to the \"documents\" folder\n",
    "documents_folder = os.path.expanduser('~/Documents')\n",
    "\n",
    "# Specify the file name\n",
    "file_name = 'covid_interventions.csv'\n",
    "\n",
    "# Construct the full file path\n",
    "file_path = os.path.join(documents_folder, file_name)\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "interventions = pd.read_csv(file_path)\n",
    "\n",
    "# Display the structure of the dataset\n",
    "interventions.info()\n",
    "\n",
    "# Display the first few rows of the dataset to get an overview\n",
    "interventions.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8b66374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 235 entries, 0 to 234\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   Scale      177 non-null    object\n",
      " 1   Measure    177 non-null    object\n",
      " 2   SD         177 non-null    object\n",
      " 3   Scale.1    235 non-null    object\n",
      " 4   Measure.1  235 non-null    object\n",
      " 5   SD.1       235 non-null    object\n",
      "dtypes: object(6)\n",
      "memory usage: 11.1+ KB\n",
      "Missing values per column:\n",
      " Scale        58\n",
      "Measure      58\n",
      "SD           58\n",
      "Scale.1       0\n",
      "Measure.1     0\n",
      "SD.1          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Drop columns with more than 50% missing values\n",
    "threshold = len(interventions) * 0.5\n",
    "interventions = interventions.dropna(thresh=threshold, axis=1)\n",
    "\n",
    "# Display the structure of the cleaned dataset\n",
    "interventions.info()\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = interventions.isnull().sum()\n",
    "print(\"Missing values per column:\\n\", missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37010eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 235 entries, 0 to 234\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   Scale      177 non-null    object\n",
      " 1   Measure    177 non-null    object\n",
      " 2   SD         177 non-null    object\n",
      " 3   Scale.1    235 non-null    object\n",
      " 4   Measure.1  235 non-null    object\n",
      " 5   SD.1       235 non-null    object\n",
      "dtypes: object(6)\n",
      "memory usage: 11.1+ KB\n",
      "None\n",
      "Numerical Columns: Index([], dtype='object')\n",
      "Series([], dtype: float64)\n"
     ]
    }
   ],
   "source": [
    "# Drop columns where all values are NaN\n",
    "interventions_cleaned = interventions.dropna(axis=1, how='all')\n",
    "\n",
    "# Display the dataframe info after dropping empty columns\n",
    "print(interventions_cleaned.info())\n",
    "\n",
    "# Check the columns to see the numerical columns available\n",
    "num_cols = interventions_cleaned.select_dtypes(include=['float64', 'int64']).columns\n",
    "print(\"Numerical Columns:\", num_cols)\n",
    "\n",
    "# Display missing values in numerical columns\n",
    "print(interventions_cleaned[num_cols].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb807410",
   "metadata": {},
   "source": [
    "Continuing the cleaning process for the new dataset, let's impute the missing values so that we can move forward with analyzation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d611e535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Scale Measure    SD                   Scale.1 Measure.1  \\\n",
      "0                      BMI    22.48   2.2                      BMI      22.45   \n",
      "1  Waist circumference (cm)   74.87  7.59  Waist circumference (cm)      74.9   \n",
      "2          Squat test (rep)   28.89   2.4          Squat test (rep)      29.4   \n",
      "3        Push-up test (rep)    9.13   4.2        Push-up test (rep)      9.53   \n",
      "4          Lunge test (rep)   31.13   5.4          Lunge test (rep)      31.4   \n",
      "\n",
      "   SD.1  \n",
      "0  2.12  \n",
      "1  7.53  \n",
      "2  2.56  \n",
      "3  4.34  \n",
      "4  6.07  \n"
     ]
    }
   ],
   "source": [
    "# Import the SimpleImputer from sklearn\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create an imputer for numerical columns with the strategy 'mean'\n",
    "imputer_num = SimpleImputer(strategy='mean')\n",
    "\n",
    "# Apply the imputer to numerical columns\n",
    "if len(num_cols) > 0:\n",
    "    interventions_cleaned[num_cols] = imputer_num.fit_transform(interventions_cleaned[num_cols])\n",
    "\n",
    "# Display the head of the cleaned dataframe\n",
    "print(interventions_cleaned.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79263a47",
   "metadata": {},
   "source": [
    "In the spirit of thoroughness, let's convert categoral variables into numerical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64a43d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical Columns: Index(['Scale', 'Measure', 'SD', 'Scale.1', 'Measure.1', 'SD.1'], dtype='object')\n",
      "   Scale  Measure   SD  Scale.1  Measure.1  SD.1\n",
      "0      9       66   52       10         74    60\n",
      "1     70      129  105       96        161   106\n",
      "2     62       87   57       86         88    65\n",
      "3     51      137   78       71        167    89\n",
      "4     34       98   92       51        117    99\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Identify categorical columns\n",
    "cat_cols = interventions_cleaned.select_dtypes(include=['object']).columns\n",
    "print(\"Categorical Columns:\", cat_cols)\n",
    "\n",
    "# Apply LabelEncoder to categorical columns\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    interventions_cleaned[col] = le.fit_transform(interventions_cleaned[col].astype(str))\n",
    "\n",
    "# Display the head of the dataframe after encoding\n",
    "print(interventions_cleaned.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc54ff9",
   "metadata": {},
   "source": [
    "Now we proceed with the training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "610df65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target variable\n",
    "X = interventions_cleaned.drop('Measure', axis=1)\n",
    "y = interventions_cleaned['Measure']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01e05a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 235 entries, 0 to 234\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count  Dtype\n",
      "---  ------     --------------  -----\n",
      " 0   Scale      235 non-null    int32\n",
      " 1   Measure    235 non-null    int32\n",
      " 2   SD         235 non-null    int32\n",
      " 3   Scale.1    235 non-null    int32\n",
      " 4   Measure.1  235 non-null    int32\n",
      " 5   SD.1       235 non-null    int32\n",
      "dtypes: int32(6)\n",
      "memory usage: 5.6 KB\n",
      "None\n",
      "   Scale  Measure   SD  Scale.1  Measure.1  SD.1\n",
      "0      9       66   52       10         74    60\n",
      "1     70      129  105       96        161   106\n",
      "2     62       87   57       86         88    65\n",
      "3     51      137   78       71        167    89\n",
      "4     34       98   92       51        117    99\n"
     ]
    }
   ],
   "source": [
    "# Display the structure and the first few rows of the cleaned dataset\n",
    "print(interventions_cleaned.info())\n",
    "print(interventions_cleaned.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee70fc92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (235, 5)\n",
      "Target shape: (235,)\n",
      "Features columns: ['Scale', 'SD', 'Scale.1', 'Measure.1', 'SD.1']\n"
     ]
    }
   ],
   "source": [
    "# Define features and target variable\n",
    "X = interventions_cleaned.drop('Measure', axis=1)  # Assuming 'Measure' is the target column\n",
    "y = interventions_cleaned['Measure']\n",
    "\n",
    "# Check the structure of X and y\n",
    "print(\"Features shape:\", X.shape)\n",
    "print(\"Target shape:\", y.shape)\n",
    "print(\"Features columns:\", X.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0cff4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (188, 5)\n",
      "X_test shape: (47, 5)\n",
      "X_train head:\n",
      "      Scale   SD  Scale.1  Measure.1  SD.1\n",
      "117     72  126       49        104    31\n",
      "155     23   11       34         45    16\n",
      "148     26   28       40        176   133\n",
      "158     32    5       46         57    11\n",
      "232     24  125       35        168   133\n",
      "Missing values in X_train:\n",
      " Scale        0\n",
      "SD           0\n",
      "Scale.1      0\n",
      "Measure.1    0\n",
      "SD.1         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check if X_train and X_test contain features\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "\n",
    "# Display first few rows of X_train to inspect the features\n",
    "print(\"X_train head:\\n\", X_train.head())\n",
    "\n",
    "# Check for missing values in X_train\n",
    "print(\"Missing values in X_train:\\n\", X_train.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e01edbe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric features: ['Scale', 'SD', 'Scale.1', 'Measure.1', 'SD.1']\n",
      "Categorical features: []\n"
     ]
    }
   ],
   "source": [
    "# Ensure numeric_features and categorical_features are correctly identified\n",
    "numeric_features = ['Scale', 'SD', 'Scale.1', 'Measure.1', 'SD.1']\n",
    "categorical_features = []  # If there are no categorical features\n",
    "\n",
    "print(\"Numeric features:\", numeric_features)\n",
    "print(\"Categorical features:\", categorical_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b81ea1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessor defined successfully\n"
     ]
    }
   ],
   "source": [
    "# Define transformers for numerical features\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Combine transformers into a preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features)\n",
    "        # If there are categorical features, add them similarly\n",
    "        # ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Preprocessor defined successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd828c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed X_train shape: (188, 5)\n",
      "Preprocessed X_train (first few rows):\n",
      " [[ 1.16455334  0.94953601  0.01080755  0.05412634 -0.90722851]\n",
      " [-0.872641   -1.65170177 -0.54332478 -0.99547043 -1.22531683]\n",
      " [-0.74791481 -1.26717097 -0.32167185  1.33499019  1.2557721 ]\n",
      " [-0.49846245 -1.78741852 -0.10001892 -0.78199312 -1.33134628]\n",
      " [-0.8310656   0.92691655 -0.50638263  1.19267199  1.2557721 ]]\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "# Disable all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Apply the preprocessor to the training data to inspect the output\n",
    "preprocessed_X_train = preprocessor.fit_transform(X_train)\n",
    "print(\"Preprocessed X_train shape:\", preprocessed_X_train.shape)\n",
    "print(\"Preprocessed X_train (first few rows):\\n\", preprocessed_X_train[:5])\n",
    "\n",
    "# Reset warnings to default behavior\n",
    "warnings.resetwarnings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d794395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.2553191489361702\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "# Disable all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Define the full pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy}\")\n",
    "\n",
    "# Reset warnings to default behavior\n",
    "warnings.resetwarnings()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55dfb9e4",
   "metadata": {},
   "source": [
    "Model accuracy is not very strong. Let's see if addressing that classification imbalance helps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4ac50ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "Best parameters: {'classifier__max_depth': None, 'classifier__max_features': None, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2}\n",
      "Best model: Pipeline(steps=[('preprocessor',\n",
      "                 ColumnTransformer(transformers=[('num',\n",
      "                                                  Pipeline(steps=[('imputer',\n",
      "                                                                   SimpleImputer()),\n",
      "                                                                  ('scaler',\n",
      "                                                                   StandardScaler())]),\n",
      "                                                  ['Scale', 'SD', 'Scale.1',\n",
      "                                                   'Measure.1', 'SD.1'])])),\n",
      "                ('classifier', DecisionTreeClassifier())])\n",
      "Model Accuracy after GridSearchCV: 0.2553191489361702\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "# Disable all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'classifier__max_depth': [None, 10, 20, 30],\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "    'classifier__min_samples_leaf': [1, 2, 4],\n",
    "    'classifier__max_features': [None, 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Create a GridSearchCV instance\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Suppress the warning\n",
    "warnings.filterwarnings(\"ignore\", message=\"The least populated class in y has only 1 member, which is less than n_splits=5.\", category=UserWarning)\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best model: {best_model}\")\n",
    "\n",
    "# Predict on the test set with the best model\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the best model\n",
    "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
    "print(f\"Model Accuracy after GridSearchCV: {accuracy_best}\")\n",
    "\n",
    "\n",
    "# Reset warnings to default behavior\n",
    "warnings.resetwarnings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42d4a7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: DecisionTree, Accuracy: 0.2765957446808511\n",
      "Model: RandomForest, Accuracy: 0.2765957446808511\n",
      "Model: GradientBoosting, Accuracy: 0.2978723404255319\n",
      "Model: LogisticRegression, Accuracy: 0.2127659574468085\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "# Define different classifiers to try\n",
    "classifiers = {\n",
    "    'DecisionTree': DecisionTreeClassifier(),\n",
    "    'RandomForest': RandomForestClassifier(),\n",
    "    'GradientBoosting': GradientBoostingClassifier(),\n",
    "    'LogisticRegression': LogisticRegression(max_iter=1000)\n",
    "}\n",
    "\n",
    "# Suppress the warning\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "for name, classifier in classifiers.items():\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Model: {name}, Accuracy: {accuracy}\")\n",
    "\n",
    "# Reset the warnings to default behavior after fitting\n",
    "warnings.resetwarnings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5aa58b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores: [0.42105263 0.42105263 0.34210526 0.37837838 0.40540541]\n",
      "Mean CV Accuracy: 0.3935988620199147\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "# Disable all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Choose the best model from previous steps, e.g., RandomForestClassifier\n",
    "best_model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "# Evaluate using cross-validation\n",
    "cv_scores = cross_val_score(best_model, X_train, y_train, cv=5)\n",
    "print(f\"Cross-Validation Scores: {cv_scores}\")\n",
    "print(f\"Mean CV Accuracy: {cv_scores.mean()}\")\n",
    "\n",
    "# Reset warnings to default behavior\n",
    "warnings.resetwarnings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99bac1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measure\n",
      "145    49\n",
      "144    15\n",
      "16      4\n",
      "6       2\n",
      "125     2\n",
      "       ..\n",
      "60      1\n",
      "54      1\n",
      "42      1\n",
      "26      1\n",
      "51      1\n",
      "Name: count, Length: 114, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check class distribution\n",
    "print(y_train.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "680a2e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.2553191489361702\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "Best parameters: {'classifier__max_depth': None, 'classifier__max_features': None, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2}\n",
      "Best model: Pipeline(steps=[('preprocessor',\n",
      "                 ColumnTransformer(transformers=[('num',\n",
      "                                                  Pipeline(steps=[('imputer',\n",
      "                                                                   SimpleImputer()),\n",
      "                                                                  ('scaler',\n",
      "                                                                   StandardScaler())]),\n",
      "                                                  ['Scale', 'SD', 'Scale.1',\n",
      "                                                   'Measure.1', 'SD.1'])])),\n",
      "                ('classifier', DecisionTreeClassifier())])\n",
      "Model Accuracy after GridSearchCV: 0.2765957446808511\n",
      "Model: DecisionTree, Accuracy: 0.2765957446808511\n",
      "Model: RandomForest, Accuracy: 0.2765957446808511\n",
      "Model: GradientBoosting, Accuracy: 0.2978723404255319\n",
      "Model: LogisticRegression, Accuracy: 0.2127659574468085\n",
      "Cross-Validation Scores: [0.42105263 0.42105263 0.34210526 0.37837838 0.40540541]\n",
      "Mean CV Accuracy: 0.3935988620199147\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "# Disable all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Define features and target variable\n",
    "X = interventions_cleaned.drop('Measure', axis=1)\n",
    "y = interventions_cleaned['Measure']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define numeric and categorical features\n",
    "numeric_features = ['Scale', 'SD', 'Scale.1', 'Measure.1', 'SD.1']\n",
    "categorical_features = []  # Add any categorical features if they exist\n",
    "\n",
    "# Define transformers for numerical features\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Combine transformers into a preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the full pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy}\")\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {\n",
    "    'classifier__max_depth': [None, 10, 20, 30],\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "    'classifier__min_samples_leaf': [1, 2, 4],\n",
    "    'classifier__max_features': [None, 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best model: {best_model}\")\n",
    "\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "accuracy_best = accuracy_score(y_test, y_pred_best)\n",
    "print(f\"Model Accuracy after GridSearchCV: {accuracy_best}\")\n",
    "\n",
    "# Trying different classifiers\n",
    "classifiers = {\n",
    "    'DecisionTree': DecisionTreeClassifier(),\n",
    "    'RandomForest': RandomForestClassifier(),\n",
    "    'GradientBoosting': GradientBoostingClassifier(),\n",
    "    'LogisticRegression': LogisticRegression(max_iter=1000)\n",
    "}\n",
    "\n",
    "for name, classifier in classifiers.items():\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', classifier)\n",
    "    ])\n",
    "    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Model: {name}, Accuracy: {accuracy}\")\n",
    "\n",
    "# Cross-validation with the best model (e.g., RandomForestClassifier)\n",
    "best_model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "cv_scores = cross_val_score(best_model, X_train, y_train, cv=5)\n",
    "print(f\"Cross-Validation Scores: {cv_scores}\")\n",
    "print(f\"Mean CV Accuracy: {cv_scores.mean()}\")\n",
    "\n",
    "# Reset warnings to default behavior\n",
    "warnings.resetwarnings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04de04da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "Model: DecisionTree, Best Parameters: {'classifier__max_depth': None, 'classifier__max_features': None, 'classifier__min_samples_leaf': 4, 'classifier__min_samples_split': 5}, Accuracy: 0.2765957446808511\n",
      "Fitting 5 folds for each of 324 candidates, totalling 1620 fits\n",
      "Model: RandomForest, Best Parameters: {'classifier__max_depth': 30, 'classifier__max_features': 'log2', 'classifier__min_samples_leaf': 2, 'classifier__min_samples_split': 10, 'classifier__n_estimators': 50}, Accuracy: 0.2978723404255319\n",
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "# Disable all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Define features and target variable\n",
    "X = interventions_cleaned.drop('Measure', axis=1)\n",
    "y = interventions_cleaned['Measure']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define numeric features\n",
    "numeric_features = ['Scale', 'SD', 'Scale.1', 'Measure.1', 'SD.1']\n",
    "categorical_features = []  # Add any categorical features if they exist\n",
    "\n",
    "# Define transformers for numerical features\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Combine transformers into a preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define classifiers with their respective hyperparameter grids\n",
    "classifiers = {\n",
    "    'DecisionTree': {\n",
    "        'model': DecisionTreeClassifier(),\n",
    "        'params': {\n",
    "            'classifier__max_depth': [None, 10, 20, 30],\n",
    "            'classifier__min_samples_split': [2, 5, 10],\n",
    "            'classifier__min_samples_leaf': [1, 2, 4],\n",
    "            'classifier__max_features': [None, 'sqrt', 'log2']\n",
    "        }\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'model': RandomForestClassifier(),\n",
    "        'params': {\n",
    "            'classifier__n_estimators': [50, 100, 200],\n",
    "            'classifier__max_depth': [None, 10, 20, 30],\n",
    "            'classifier__min_samples_split': [2, 5, 10],\n",
    "            'classifier__min_samples_leaf': [1, 2, 4],\n",
    "            'classifier__max_features': [None, 'sqrt', 'log2']\n",
    "        }\n",
    "    },\n",
    "    'GradientBoosting': {\n",
    "        'model': GradientBoostingClassifier(),\n",
    "        'params': {\n",
    "            'classifier__n_estimators': [50, 100, 200],\n",
    "            'classifier__learning_rate': [0.01, 0.1, 0.2],\n",
    "            'classifier__max_depth': [3, 5, 7]\n",
    "        }\n",
    "    },\n",
    "    'LogisticRegression': {\n",
    "        'model': LogisticRegression(max_iter=1000),\n",
    "        'params': {\n",
    "            'classifier__C': [0.01, 0.1, 1, 10, 100],\n",
    "            'classifier__solver': ['lbfgs', 'saga']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize a variable to store the best model and its score\n",
    "best_model = None\n",
    "best_score = 0\n",
    "\n",
    "# Iterate through classifiers and perform GridSearchCV\n",
    "for name, clf_info in classifiers.items():\n",
    "    pipeline = ImbPipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('oversampler', RandomOverSampler(random_state=42)),\n",
    "        ('classifier', clf_info['model'])\n",
    "    ])\n",
    "    \n",
    "    grid_search = GridSearchCV(pipeline, clf_info['params'], cv=5, n_jobs=-1, verbose=2)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = grid_search.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Model: {name}, Best Parameters: {grid_search.best_params_}, Accuracy: {accuracy}\")\n",
    "    \n",
    "    if accuracy > best_score:\n",
    "        best_score = accuracy\n",
    "        best_model = grid_search.best_estimator_\n",
    "\n",
    "# Cross-validation with the best model\n",
    "cv_scores = cross_val_score(best_model, X_train, y_train, cv=5)\n",
    "print(f\"Cross-Validation Scores: {cv_scores}\")\n",
    "print(f\"Mean CV Accuracy: {cv_scores.mean()}\")\n",
    "\n",
    "# Reset warnings to default behavior\n",
    "warnings.resetwarnings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76de71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Suppress warnings for clarity\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Define oversampler with random state for reproducibility\n",
    "oversampler = SMOTE(random_state=42)\n",
    "\n",
    "# Iterate through classifiers and perform GridSearchCV\n",
    "for name, clf_info in classifiers.items():\n",
    "    print(f\"Processing {name} classifier...\")\n",
    "    \n",
    "    # Option 1: Use SMOTE for oversampling (replace with class weighting if preferred)\n",
    "    pipeline = ImbPipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('oversampler', oversampler),\n",
    "        ('classifier', clf_info['model'])\n",
    "    ])\n",
    "\n",
    "    grid_search = GridSearchCV(pipeline, clf_info['params'], cv=5, n_jobs=-1, verbose=2)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Grid search completed for {name} classifier.\")\n",
    "    \n",
    "    y_pred = grid_search.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Model: {name}, Best Parameters: {grid_search.best_params_}, Accuracy: {accuracy}\")\n",
    "\n",
    "    if accuracy > best_score:\n",
    "        best_score = accuracy\n",
    "        best_model = grid_search.best_estimator_\n",
    "\n",
    "# Cross-validation with the best model\n",
    "cv_scores = cross_val_score(best_model, X_train, y_train, cv=5)\n",
    "print(f\"Cross-Validation Scores: {cv_scores}\")\n",
    "print(f\"Mean CV Accuracy: {cv_scores.mean()}\")\n",
    "\n",
    "# Reset warnings to default behavior\n",
    "warnings.resetwarnings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b96482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming interventions_cleaned is already defined\n",
    "\n",
    "# Split data into training, validation, and test sets (consider 70% train, 15% validation, 15% test)\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X_cleaned, y_cleaned, test_size=0.3, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define transformers and preprocessor (same as before)\n",
    "# ... (your existing preprocessor code) ...\n",
    "\n",
    "# Use the best model identified from GridSearchCV\n",
    "best_model = ...  # Replace with the actual best_model object from GridSearchCV\n",
    "\n",
    "# Train the best model on the entire training set\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate on validation set (optional, for hyperparameter tuning)\n",
    "val_pred = best_model.predict(X_val)\n",
    "val_accuracy = accuracy_score(y_val, val_pred)\n",
    "print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "# Evaluate on hold-out test set\n",
    "test_pred = best_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, test_pred)\n",
    "print(f\"Test Accuracy: {test_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054037f8",
   "metadata": {},
   "source": [
    "Inferences about Intervention Effectiveness (with Limitations):\n",
    "\n",
    "While definitively concluding about the effectiveness of interventions is outside the scope of this initial model, we can glean some tentative inferences. Feature importance analysis from the final model can reveal which intervention-related features have the strongest influence on the target variable. Interventions with high importance might warrant further investigation for potential effectiveness. However, it's crucial to remember that correlation does not imply causation. Other factors could be influencing the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09de4aef",
   "metadata": {},
   "source": [
    "Next Steps: Modeling for Intervention Effectiveness\n",
    "\n",
    "Here in the Pre-processing and training phase, we have laid the groundwork for a more in-depth analysis of intervention effectiveness in the next phase of modeling. Here is how we willproceed:\n",
    "\n",
    "Target Variable Selection: We will define a new target variable specifically focused on assessing intervention effectiveness. This could involve metrics like reduced infection rates, improved economic indicators, or other relevant measures tied to the interventions' goals.\n",
    "\n",
    "Intervention Features: We will ensure intervention data is included as features in the next model. This might involve coding interventions as categorical variables or creating numerical representations based on their intensity or duration.\n",
    "\n",
    "Model Selection: Based on the findings from the current workflow (e.g., best performing model), we will choose an appropriate model for the new target variable and intervention analysis.\n",
    "\n",
    "Evaluation Metrics: We will utilize relevant evaluation metrics to assess the model's performance in predicting intervention effectiveness. This might involve metrics like precision, recall, or F1-score for classification tasks, or mean squared error (MSE) for regression tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5148c052",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "\n",
    "This study successfully developed a foundational machine learning model. By analyzing feature importance, defining an intervention-specific target variable, incorporating intervention data as features, and using relevant evaluation metrics in the next phase, we can gain valuable insights into how well the interventions work within this dataset. This paves the way for a more robust and targeted analysis of intervention effectiveness."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
